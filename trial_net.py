# -*- coding: utf-8 -*-
"""Trial Net.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yJdkfVr8hil4C9Rp2yFOPFKdlxT3lx4W
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import math

PV_data = pd.read_csv('PV_data.csv', header = None).to_numpy()

X = PV_data[0:4, :]
Y = PV_data[4:, :]
Y[1, :] = Y[1, :]*(-1)
num_data_points = X.shape[1]


shuffled_cols = np.random.permutation(num_data_points)

percent_training = 80/100
percent_dev = 15/100
percent_test = 5/100

m_train = math.floor(percent_training*num_data_points)
m_dev = math.floor(percent_dev*num_data_points)
m_test = num_data_points - m_train - m_dev

X_train = X[:, shuffled_cols[0:m_train]]
X_dev = X[:, shuffled_cols[m_train: m_train + m_dev]]
X_test = X[:, shuffled_cols[m_train + m_dev: ]]

Y_train = Y[:, shuffled_cols[0:m_train]]
Y_dev = Y[:, shuffled_cols[m_train: m_train + m_dev]]
Y_test = Y[:, shuffled_cols[m_train + m_dev: ]]

X_train_mean = np.mean(X_train, axis = 1, keepdims=True)
X_train_std = np.std(X_train, axis = 1, keepdims=True)

X_train_n = (X_train-X_train_mean)/X_train_std
X_dev_n = (X_dev - X_train_mean)/X_train_std
X_test_n = (X_test - X_train_mean)/X_train_std

num_hidden_layers = 2
num_layers = num_hidden_layers + 1
  
np.random.seed(1986)
n_x = X_train_n.shape[0]
n_h1 = 10
n_h2 = 8
n_y = Y_train.shape[0]
num_units_list = [n_x, n_h1, n_h2, n_y]

act_h1 = "relu"
act_h2 = "relu"
act_L = "linear"
act_fun_list = [act_h1, act_h2, act_L]

lambd = 0

learning_rate = 0.1
num_iters = 500

def initialize_params(n_input, n_output, act_fun = 'relu'):

  if act_fun == 'sigmoid' or act_fun == 'tanh':
    W = np.random.randn(n_output, n_input)*np.sqrt(1/n_input)
  elif act_fun == 'relu' or act_fun == 'leaky_relu':
    W = np.random.randn(n_output, n_input)*np.sqrt(2/n_input)
  else:
    W = np.random.randn(n_output, n_input)

  b = np.zeros((n_output, 1))

  return W, b

def initialize_params_nn(num_units_list, act_fun_list):

  num_layers = len(act_fun_list)
  parameters = {}
  for l in range(num_layers):
    parameters['W' + str(l+1)], parameters['b' + str(l+1)] = initialize_params(num_units_list[l], num_units_list[l+1], act_fun_list[l])

  return parameters

def activ(z, act_fun = 'relu'):

  if act_fun == 'relu':
    a = np.maximum(0, z)
  elif act_fun == 'leaky_relu':
    a = np.maximum(0.01*z, z)
  elif act_fun == 'sigmoid':
    a = 1/(1+np.exp(-z))
  elif act_fun == 'tanh':
    a = (np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z))
  elif act_fun == 'linear':
    a = z

  return a

def forward_prop(input_data, W, b, act_fun = 'relu'):

  lin_output = np.dot(W, input_data) + b
  act_output = activ(lin_output, act_fun)
  return lin_output, act_output

def forward_prop_nn(X, parameters, act_fun_list):

  num_layers = len(act_fun_list)
  cache = {}
  current_layer_data = X
  for l in range(num_layers):
    cache['z' + str(l+1)], cache['a' + str(l+1)] = forward_prop(current_layer_data, parameters['W' + str(l+1)], parameters['b' + str(l+1)], act_fun_list[l])
    current_layer_data = cache['a' + str(l+1)]

  AL = cache['a' + str(num_layers)]
  cache['a0'] = X
  return cache, AL

def cost_func(AL, Y):

  num_data_points = Y.shape[1]
  cost = (0.5/num_data_points)*np.sum(np.square(AL - Y))
  np.squeeze(cost)
  return cost

def act_fun_deriv(z, act_fun):

  if act_fun == 'relu':
    act_fun_grad = (z>0).astype(int)
  elif act_fun == 'sigmoid':
    act_fun_grad = activ(z, 'sigmoid')*(1-activ(z, 'sigmoid'))
  elif act_fun == 'linear':
    act_fun_grad = np.ones(z.shape)

  return act_fun_grad

def backprop_nn(AL, Y, act_fun_list, cache, parameters):

  num_layers = len(act_fun_list)
  num_data_points = Y.shape[1]
  grads = {}
  dA3 = (1/num_data_points)*(AL - Y)
  dZ3 = dA3*act_fun_deriv(cache['z3'], 'linear')
  grads['dW3'] = (1/num_data_points)*np.dot(dZ3, cache['a2'].T)
  grads['db3'] = (1/num_data_points)*np.sum(dZ3, keepdims=True, axis = 1)
  dA2 = np.dot(parameters['W3'].T, dZ3)
  dZ2 = dA2*act_fun_deriv(cache['z2'], 'relu')
  grads['dW2'] = (1/num_data_points)*np.dot(dZ2, cache['a1'].T)
  grads['db2'] = (1/num_data_points)*np.sum(dZ2, keepdims=True, axis = 1)
  dA1 = np.dot(parameters['W2'].T, dZ2)
  dZ1 = dA1*act_fun_deriv(cache['z1'], 'relu')
  grads['dW1'] = (1/num_data_points)*np.dot(dZ1, cache['a0'].T)
  grads['db1'] = (1/num_data_points)*np.sum(dZ1, keepdims=True, axis = 1)


  return grads

def update_params(parameters, grads, learning_rate, num_layers):

  for l in range(num_layers):
    parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate*grads["dW" + str(l+1)]
    parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate*grads["db" + str(l+1)]

  return parameters

parameters = initialize_params_nn(num_units_list, act_fun_list)
cost_tracker = np.zeros(20000)
for iter in range(20000):
  cache, AL = forward_prop_nn(X_train_n, parameters, act_fun_list)
  cost_tracker[iter] = cost_func(AL, Y_train)
  grads = backprop_nn(AL, Y_train, act_fun_list, cache, parameters)
  parameters = update_params(parameters, grads, learning_rate, num_layers)

plt.plot(np.arange(20000), cost_tracker)
plt.show()